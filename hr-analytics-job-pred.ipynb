{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2334549,"sourceType":"datasetVersion","datasetId":1409252}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-09T12:23:45.926198Z","iopub.execute_input":"2023-11-09T12:23:45.927382Z","iopub.status.idle":"2023-11-09T12:23:45.938322Z","shell.execute_reply.started":"2023-11-09T12:23:45.927334Z","shell.execute_reply":"2023-11-09T12:23:45.936939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load dataset","metadata":{}},{"cell_type":"code","source":"# RUN THIS CELL TO IMPORT YOUR DATA. \n\n# Load dataset into a dataframe\n### YOUR CODE HERE ###\ndf0 = pd.read_csv(\"/kaggle/input/hr-analytics-and-job-prediction/HR_comma_sep.csv\")\n\n\n# Display first few rows of the dataframe\n### YOUR CODE HERE ###\ndf0.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:45.940608Z","iopub.execute_input":"2023-11-09T12:23:45.941537Z","iopub.status.idle":"2023-11-09T12:23:45.986528Z","shell.execute_reply.started":"2023-11-09T12:23:45.941500Z","shell.execute_reply":"2023-11-09T12:23:45.985119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration (EDA and data cleaning)","metadata":{}},{"cell_type":"code","source":"#Basic information about data\ndf0.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:45.988496Z","iopub.execute_input":"2023-11-09T12:23:45.988903Z","iopub.status.idle":"2023-11-09T12:23:46.024920Z","shell.execute_reply.started":"2023-11-09T12:23:45.988868Z","shell.execute_reply":"2023-11-09T12:23:46.023626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Basic descriptive statistics about data\ndf0.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:46.028060Z","iopub.execute_input":"2023-11-09T12:23:46.028957Z","iopub.status.idle":"2023-11-09T12:23:46.074972Z","shell.execute_reply.started":"2023-11-09T12:23:46.028900Z","shell.execute_reply":"2023-11-09T12:23:46.073740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:46.076406Z","iopub.execute_input":"2023-11-09T12:23:46.076760Z","iopub.status.idle":"2023-11-09T12:23:46.085006Z","shell.execute_reply.started":"2023-11-09T12:23:46.076731Z","shell.execute_reply":"2023-11-09T12:23:46.083819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the columns have misspeling and for some reason few starts with a capital letter, for convinience we will rename those columns","metadata":{}},{"cell_type":"code","source":"#Renaming the columns\ndf0 = df0.rename(columns={'time_spend_company':'years_spend_company',\n                    'Work_accident':'work_accident',\n                   'Department':'department',\n                    'average_montly_hours': 'average_monthly_hours'})\n#checking new names\ndf0.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:46.086544Z","iopub.execute_input":"2023-11-09T12:23:46.087478Z","iopub.status.idle":"2023-11-09T12:23:46.099798Z","shell.execute_reply.started":"2023-11-09T12:23:46.087440Z","shell.execute_reply":"2023-11-09T12:23:46.098455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check missing values","metadata":{}},{"cell_type":"code","source":"df0.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:23:46.102038Z","iopub.execute_input":"2023-11-09T12:23:46.103376Z","iopub.status.idle":"2023-11-09T12:23:46.120444Z","shell.execute_reply.started":"2023-11-09T12:23:46.103315Z","shell.execute_reply":"2023-11-09T12:23:46.119081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing data","metadata":{}},{"cell_type":"markdown","source":"## Check duplicates","metadata":{}},{"cell_type":"code","source":"df0.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:24:50.005605Z","iopub.execute_input":"2023-11-09T12:24:50.006027Z","iopub.status.idle":"2023-11-09T12:24:50.023501Z","shell.execute_reply.started":"2023-11-09T12:24:50.005992Z","shell.execute_reply":"2023-11-09T12:24:50.022270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, thats a lot, should inspect further","metadata":{}},{"cell_type":"code","source":"df0[df0.duplicated()].head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:29:45.012466Z","iopub.execute_input":"2023-11-09T12:29:45.012901Z","iopub.status.idle":"2023-11-09T12:29:45.039204Z","shell.execute_reply.started":"2023-11-09T12:29:45.012866Z","shell.execute_reply":"2023-11-09T12:29:45.037633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is very little chance for appearing 2 same rows on random when there is so many variable, so i think we shoud drop duplicates","metadata":{}},{"cell_type":"code","source":"#drop duplicates and save clear data in a new variable\ndf = df0[~df0.duplicated()]\n#check first few rows of new dataframe\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:32:18.510600Z","iopub.execute_input":"2023-11-09T12:32:18.511469Z","iopub.status.idle":"2023-11-09T12:32:18.536860Z","shell.execute_reply.started":"2023-11-09T12:32:18.511422Z","shell.execute_reply":"2023-11-09T12:32:18.535704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check outliers","metadata":{}},{"cell_type":"code","source":"#it's easier to detect outliers by creating visualization\n\nsns.boxplot(df['years_spend_company'], orient='h')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:36:07.701062Z","iopub.execute_input":"2023-11-09T12:36:07.701485Z","iopub.status.idle":"2023-11-09T12:36:07.896206Z","shell.execute_reply.started":"2023-11-09T12:36:07.701452Z","shell.execute_reply":"2023-11-09T12:36:07.894865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine the number of rows containing outliers\n\npercentile25 = df['years_spend_company'].quantile(0.25)\npercentile75 = df['years_spend_company'].quantile(0.75)\n\n#interquantile range:\niqr = percentile75 - percentile25\nlower_limit = percentile25 - 1.5*iqr\nupper_limit = percentile75 + 1.5*iqr\n\nprint('Lower limit: ', lower_limit)\nprint('Upper limit: ', upper_limit)\n\noutliers = df[(df['years_spend_company'] < lower_limit)|(df['years_spend_company'] > upper_limit)]\nprint('Number of outliers: ', len(outliers))","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:48:47.235882Z","iopub.execute_input":"2023-11-09T12:48:47.236320Z","iopub.status.idle":"2023-11-09T12:48:47.249669Z","shell.execute_reply.started":"2023-11-09T12:48:47.236288Z","shell.execute_reply":"2023-11-09T12:48:47.248226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Certain types of models are more sensitive to outliers than others. On the model building stage i should consider that dataset has some outliers","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"# numbers of people who left vs. stayed\nleft_v_stay = df['left'].value_counts()\nprint(left_v_stay)\n#percentages of people who left vs. stayed\nprint('Stayed: ', round(left_v_stay[0]/len(df['left'])*100,3))\nprint('left: ', round(left_v_stay[1]/len(df['left'])*100,3))","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:50:46.551696Z","iopub.execute_input":"2023-11-09T12:50:46.552289Z","iopub.status.idle":"2023-11-09T12:50:46.564801Z","shell.execute_reply.started":"2023-11-09T12:50:46.552241Z","shell.execute_reply":"2023-11-09T12:50:46.563416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is unbalanced, approximately 16.6% employees in this dataset left. But it is not extremely unbalanced and we can model without any class rebalancing","metadata":{}},{"cell_type":"markdown","source":"### Data visualizations","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(15,8))\n\nsns.scatterplot(x=df['number_project'], y=df['satisfaction_level'], hue=df['left'], ax=ax[0])\nax[0].set_title('Satisfaction level vs number of projects')\nsns.histplot(data=df, x='number_project', hue='left',multiple='dodge',shrink=3)\nax[1].set_title('Number of project histogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:52:05.647588Z","iopub.execute_input":"2023-11-09T12:52:05.648826Z","iopub.status.idle":"2023-11-09T12:52:06.923583Z","shell.execute_reply.started":"2023-11-09T12:52:05.648786Z","shell.execute_reply":"2023-11-09T12:52:06.922247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting almost everyone who left has low satisfaction level and almost every one who had 7 project are left employees\n\nPeople with 3 projects have the lowes number of leaving employees","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(data=df, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha =0.5)\nplt.axvline(x=160, color='r', label='160 hours/mo.', ls='--')\nplt.legend(labels=['160 hours/mo.', 'left', 'stayed'])\nplt.title('mothly hours by satisfaction level')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:56:51.462045Z","iopub.execute_input":"2023-11-09T12:56:51.462558Z","iopub.status.idle":"2023-11-09T12:56:52.762371Z","shell.execute_reply.started":"2023-11-09T12:56:51.462522Z","shell.execute_reply":"2023-11-09T12:56:52.761082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* data points are grouped in rectangles this could indicate that data is synthetic\n* Also this plot shows us that a lot of left employees severly overworked and big group worked just a little bit less than average\n* But most of left people have low satisfaction level","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (16,9))\n\nsns.boxplot(data=df, x='years_spend_company', y='satisfaction_level', hue='left', ax=ax[0])\nax[0].set_title('Satisfaction by years spent in company')\n\nsns.histplot(data=df, x='years_spend_company', hue='left', multiple='dodge', shrink=5, ax=ax[1])\nax[1].set_title('Years spent in company histogram')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:59:56.954482Z","iopub.execute_input":"2023-11-09T12:59:56.954946Z","iopub.status.idle":"2023-11-09T12:59:58.421220Z","shell.execute_reply.started":"2023-11-09T12:59:56.954911Z","shell.execute_reply":"2023-11-09T12:59:58.419987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Employees who left fall into two general categories: dissatisfied employees with shorter years worked for company and very satisfied employees with medium-length years worked for company.\n* Four-year employees who left seem to have an unusually low satisfaction level.\n* The histogram shows that there are relatively few longer years_spend_company employees. It's possible that they're the higher-ranking, higher-paid employees.","metadata":{}},{"cell_type":"code","source":"df.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:04:45.912439Z","iopub.execute_input":"2023-11-09T13:04:45.912839Z","iopub.status.idle":"2023-11-09T13:04:45.932860Z","shell.execute_reply.started":"2023-11-09T13:04:45.912804Z","shell.execute_reply":"2023-11-09T13:04:45.931641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the mean and median satisfaction scores of employees who left are lower than those of employees who stayed.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(16,9))\n#long and short time working employees\nlong_yrs_comp = df[df['years_spend_company'] > 6]\nshort_yrs_comp = df[df['years_spend_company'] <=6]\n#plot short time working employees and their salaries\nsns.histplot(data=short_yrs_comp, x='years_spend_company', hue='salary', hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=3, ax=ax[0])\nax[0].set_title('salary of short-time working employees')\n\nsns.histplot(data=long_yrs_comp, x='years_spend_company', hue='salary', hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=2, ax=ax[1])\nax[1].set_title('salary of long-time working employees')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:06:29.940262Z","iopub.execute_input":"2023-11-09T13:06:29.940675Z","iopub.status.idle":"2023-11-09T13:06:31.053887Z","shell.execute_reply.started":"2023-11-09T13:06:29.940636Z","shell.execute_reply":"2023-11-09T13:06:31.052601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Very strange observation occured, there is no one who worked for 9 years in this company. \n* And as we can see employees who spent more years for this company proportionally tend to have higher salaries","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,3))\n\nsns.scatterplot(data=df, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.5)\nplt.axvline(x=160, color='r', label='160 hrs/mo.', ls='--')\nplt.legend(['160 hrs/mo.', 'left', 'stayed'])\nplt.title('Monthly hours vs promotion')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:08:31.343729Z","iopub.execute_input":"2023-11-09T13:08:31.344213Z","iopub.status.idle":"2023-11-09T13:08:32.209767Z","shell.execute_reply.started":"2023-11-09T13:08:31.344154Z","shell.execute_reply":"2023-11-09T13:08:32.208476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Very few people who were promoted left the company.\n* And almost everyone who worked more than 280 hours and was not promoted left","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,9))\n\nsns.heatmap(df.drop(columns=['department', 'salary']).corr(), vmin=-1, vmax=1, annot=True, cmap=sns.cubehelix_palette(as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:12:13.316911Z","iopub.execute_input":"2023-11-09T13:12:13.317342Z","iopub.status.idle":"2023-11-09T13:12:14.066882Z","shell.execute_reply.started":"2023-11-09T13:12:13.317309Z","shell.execute_reply":"2023-11-09T13:12:14.065633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation heatmap confirms that the number of projects, monthly hours, and evaluation scores all have some positive correlation with each other, and whether an employee leaves is negatively correlated with their satisfaction level.","metadata":{}},{"cell_type":"markdown","source":"Leaving is tied to longer working hours, many projects, and generally lower satisfaction levels. It can be ungratifying to work long hours and not receive promotions or good evaluation scores. There's a sizeable group of employees at this company who are probably burned out.","metadata":{}},{"cell_type":"markdown","source":"# Model Building\n\n### Identify types of models\nSince the variable we need to predict (whether an employees leaves the company) is categorical,\nI choose to build 2 Tree-based machine learning models compare them and choose the best one.","metadata":{}},{"cell_type":"markdown","source":"## Modeling\nFirstly i would like to make some new features that might help my model.","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:17:20.831324Z","iopub.execute_input":"2023-11-09T13:17:20.832660Z","iopub.status.idle":"2023-11-09T13:17:20.841221Z","shell.execute_reply.started":"2023-11-09T13:17:20.832585Z","shell.execute_reply":"2023-11-09T13:17:20.839743Z"}}},{"cell_type":"code","source":"#feature engeneering\ndf['hard_worker'] = np.where(df['average_monthly_hours'] > 300, 1, 0)\n\ndf['load'] = df['average_monthly_hours']*df['last_evaluation']\n\ndf['hours_per_project'] = df['average_monthly_hours']/df['number_project']\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:17:31.043433Z","iopub.execute_input":"2023-11-09T13:17:31.043818Z","iopub.status.idle":"2023-11-09T13:17:31.072467Z","shell.execute_reply.started":"2023-11-09T13:17:31.043789Z","shell.execute_reply":"2023-11-09T13:17:31.071383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to convert our categorical variablte to numeric, because model can only understand numeric ones","metadata":{}},{"cell_type":"code","source":"df['salary'].replace({'low':1,\n                       'medium':2,\n                       'high':3},\n                     inplace=True)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:18:24.837154Z","iopub.execute_input":"2023-11-09T13:18:24.837596Z","iopub.status.idle":"2023-11-09T13:18:24.871128Z","shell.execute_reply.started":"2023-11-09T13:18:24.837561Z","shell.execute_reply":"2023-11-09T13:18:24.869406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.get_dummies(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:18:54.160651Z","iopub.execute_input":"2023-11-09T13:18:54.161104Z","iopub.status.idle":"2023-11-09T13:18:54.193567Z","shell.execute_reply.started":"2023-11-09T13:18:54.161068Z","shell.execute_reply":"2023-11-09T13:18:54.191949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data split\nNow we are ready to build a model but first we need to split our data into train/validation/test sets(60,20,20)","metadata":{}},{"cell_type":"code","source":"#Separate our X and y variables wich isolate features and target variables\n\ny = df['left']\n\nX = df.copy()\nX = X.drop(columns=['left'])\n\n#Split into train validation and test sets\n\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, stratify=y_tr, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:19:55.537681Z","iopub.execute_input":"2023-11-09T13:19:55.538142Z","iopub.status.idle":"2023-11-09T13:19:55.562735Z","shell.execute_reply.started":"2023-11-09T13:19:55.538093Z","shell.execute_reply":"2023-11-09T13:19:55.561373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verify the number of rows in each of the sets.","metadata":{}},{"cell_type":"code","source":"print('Train:', X_train.shape, y_train.shape)\nprint('test:', X_test.shape, y_test.shape)\nprint('validation:', X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:19:58.323773Z","iopub.execute_input":"2023-11-09T13:19:58.324230Z","iopub.status.idle":"2023-11-09T13:19:58.331352Z","shell.execute_reply.started":"2023-11-09T13:19:58.324194Z","shell.execute_reply":"2023-11-09T13:19:58.329849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBClassifier\nWe begin with using GridSearchCV to tune the model","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(objective='binary:logistic',random_state=42)\n\ncv_params = {'n_estimators': [150,300,500],\n            'max_depth': [3,4,5],\n            'min_child_weight': [0.5,1,1.5],\n            'learning_rate':[0.05,0.1,0.2],\n            }\n\n#dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\nxgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:21:36.924209Z","iopub.execute_input":"2023-11-09T13:21:36.924634Z","iopub.status.idle":"2023-11-09T13:21:36.932424Z","shell.execute_reply.started":"2023-11-09T13:21:36.924600Z","shell.execute_reply":"2023-11-09T13:21:36.931324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to fit the model","metadata":{}},{"cell_type":"code","source":"%%time\nxgb_cv.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:21:47.277153Z","iopub.execute_input":"2023-11-09T13:21:47.277562Z","iopub.status.idle":"2023-11-09T13:29:16.726266Z","shell.execute_reply.started":"2023-11-09T13:21:47.277530Z","shell.execute_reply":"2023-11-09T13:29:16.725022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine our best average score","metadata":{}},{"cell_type":"code","source":"xgb_cv.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:32:33.930777Z","iopub.execute_input":"2023-11-09T13:32:33.931289Z","iopub.status.idle":"2023-11-09T13:32:33.939246Z","shell.execute_reply.started":"2023-11-09T13:32:33.931247Z","shell.execute_reply":"2023-11-09T13:32:33.937938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best combination of hyperparameters","metadata":{}},{"cell_type":"code","source":"xgb_cv.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:32:36.756420Z","iopub.execute_input":"2023-11-09T13:32:36.756836Z","iopub.status.idle":"2023-11-09T13:32:36.765638Z","shell.execute_reply.started":"2023-11-09T13:32:36.756805Z","shell.execute_reply":"2023-11-09T13:32:36.764332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Could be useful to try different hyperparameters to tune model but for now i would leave it like this","metadata":{}},{"cell_type":"code","source":"def make_results(model_name:str, model_object, metric:str):\n    '''\n    model_name: is the name that you want to give to your model in the output table\n    model_object: a fit GridSearchCV object\n    metric: could be precision, recall, accuracy or f1\n    \n    returns pandas DataFrame with the precision, recall, accuracy and f1 scores \n    for the model with the best 'metric' score across all validation folds\n    '''\n    metric_dict = {'precision':'mean_test_precision',\n                  'recall':'mean_test_recall',\n                  'f1':'mean_test_f1',\n                  'accuracy':'mean_test_accuracy'}\n    # we need to get all the results from the CV\n    cv_results = pd.DataFrame(model_object.cv_results_)\n    \n    #now we need to isolate row with the best mean metric score\n    best_result = cv_results.loc[cv_results[metric_dict[metric]].idxmax(),:]\n    \n    precision = best_result.mean_test_precision\n    recall = best_result.mean_test_recall\n    f1 = best_result.mean_test_f1\n    accuracy = best_result.mean_test_accuracy\n    \n    table = pd.DataFrame({'model_name':[model_name],\n                         'precision':[precision],\n                         'recall':[recall],\n                         'f1':[f1],\n                         'accuracy':[accuracy]},)\n    return table","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:32:39.556236Z","iopub.execute_input":"2023-11-09T13:32:39.556879Z","iopub.status.idle":"2023-11-09T13:32:39.566338Z","shell.execute_reply.started":"2023-11-09T13:32:39.556844Z","shell.execute_reply":"2023-11-09T13:32:39.564550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = make_results('XGBClassifier', xgb_cv, 'recall')\nresults","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:32:45.348912Z","iopub.execute_input":"2023-11-09T13:32:45.349363Z","iopub.status.idle":"2023-11-09T13:32:45.369852Z","shell.execute_reply.started":"2023-11-09T13:32:45.349324Z","shell.execute_reply":"2023-11-09T13:32:45.368601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the scores are pretty high, i guess it would be helpfull to spend more time tunning the model to make recall score even higher","metadata":{}},{"cell_type":"markdown","source":"## Random Forest\nWe begin with using GridSearchCV to tune the model","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=42)\n\ncv_params_rf = {'max_depth': [3,5,None],\n             'max_features': [\"sqrt\"],\n             'max_samples': [0.8, 0.9],\n             'min_samples_leaf': [1,2,4],\n             'min_samples_split': [3,4,6],\n             'n_estimators': [50,75,100],\n             }\n\nscoring = {'accuracy', 'precision', 'recall', 'f1'}\n\nrf_cv = GridSearchCV(rf, cv_params_rf, scoring=scoring, cv=5, refit='recall')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:33:49.743404Z","iopub.execute_input":"2023-11-09T13:33:49.743824Z","iopub.status.idle":"2023-11-09T13:33:49.752052Z","shell.execute_reply.started":"2023-11-09T13:33:49.743791Z","shell.execute_reply":"2023-11-09T13:33:49.750427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrf_cv.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:33:53.117449Z","iopub.execute_input":"2023-11-09T13:33:53.117941Z","iopub.status.idle":"2023-11-09T13:39:50.343932Z","shell.execute_reply.started":"2023-11-09T13:33:53.117903Z","shell.execute_reply":"2023-11-09T13:39:50.342536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cv.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:40:18.265577Z","iopub.execute_input":"2023-11-09T13:40:18.266015Z","iopub.status.idle":"2023-11-09T13:40:18.273815Z","shell.execute_reply.started":"2023-11-09T13:40:18.265982Z","shell.execute_reply":"2023-11-09T13:40:18.272682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cv.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:40:20.565695Z","iopub.execute_input":"2023-11-09T13:40:20.566092Z","iopub.status.idle":"2023-11-09T13:40:20.573682Z","shell.execute_reply.started":"2023-11-09T13:40:20.566059Z","shell.execute_reply":"2023-11-09T13:40:20.572512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_res = make_results('RandomForest', rf_cv, 'recall')\nresults = pd.concat([results, rf_res])\nresults","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:40:42.776073Z","iopub.execute_input":"2023-11-09T13:40:42.776525Z","iopub.status.idle":"2023-11-09T13:40:42.794815Z","shell.execute_reply.started":"2023-11-09T13:40:42.776491Z","shell.execute_reply":"2023-11-09T13:40:42.793812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing to models we can see that XGBoost model performs a bit better on training data, now let's compare them on validation dataset","metadata":{}},{"cell_type":"code","source":"xgb_y_pred = xgb_cv.best_estimator_.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:03.510504Z","iopub.execute_input":"2023-11-09T13:41:03.510942Z","iopub.status.idle":"2023-11-09T13:41:03.558534Z","shell.execute_reply.started":"2023-11-09T13:41:03.510908Z","shell.execute_reply":"2023-11-09T13:41:03.557509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_score(model_name:str, preds, y_test_data):\n    \n    \n    precision = precision_score(y_test_data, preds)\n    recall = recall_score(y_test_data, preds)\n    f1 = f1_score(y_test_data, preds)\n    accuracy = accuracy_score(y_test_data, preds)\n    \n    table = pd.DataFrame({'model_name':[model_name],\n                         'precision':[precision],\n                         'recall':[recall],\n                         'f1':[f1],\n                         'accuracy':[accuracy]},)\n    return table","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:05.926907Z","iopub.execute_input":"2023-11-09T13:41:05.927370Z","iopub.status.idle":"2023-11-09T13:41:05.935810Z","shell.execute_reply.started":"2023-11-09T13:41:05.927332Z","shell.execute_reply":"2023-11-09T13:41:05.934253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_val_scores = get_test_score('XGB_val', xgb_y_pred, y_val)\n\nresults = pd.concat([results, xgb_val_scores])","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:08.458913Z","iopub.execute_input":"2023-11-09T13:41:08.459346Z","iopub.status.idle":"2023-11-09T13:41:08.476316Z","shell.execute_reply.started":"2023-11-09T13:41:08.459313Z","shell.execute_reply":"2023-11-09T13:41:08.475312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_y_preds = rf_cv.best_estimator_.predict(X_val)\n\nrf_val_scores = get_test_score('RF_val', rf_y_preds, y_val)\nresults = pd.concat([results, rf_val_scores])\nresults","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:12.937110Z","iopub.execute_input":"2023-11-09T13:41:12.937562Z","iopub.status.idle":"2023-11-09T13:41:13.008955Z","shell.execute_reply.started":"2023-11-09T13:41:12.937527Z","shell.execute_reply":"2023-11-09T13:41:13.007851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGB model made even better prediction on validation dataset\nNow let's fit our best model with test data","metadata":{}},{"cell_type":"code","source":"y_pred = xgb_cv.best_estimator_.predict(X_test)\n\nxgb_final_score = get_test_score('XGB_best', y_pred, y_test)\n\nresults = pd.concat([results,xgb_final_score])\nresults","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:41.695731Z","iopub.execute_input":"2023-11-09T13:41:41.696191Z","iopub.status.idle":"2023-11-09T13:41:41.738679Z","shell.execute_reply.started":"2023-11-09T13:41:41.696103Z","shell.execute_reply":"2023-11-09T13:41:41.737370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Construct confusion matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,y_pred)\n\ndisp = ConfusionMatrixDisplay(cm, display_labels=xgb_cv.classes_)\ndisp.plot()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:41:52.050106Z","iopub.execute_input":"2023-11-09T13:41:52.050503Z","iopub.status.idle":"2023-11-09T13:41:52.427022Z","shell.execute_reply.started":"2023-11-09T13:41:52.050474Z","shell.execute_reply":"2023-11-09T13:41:52.426052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model predicts more False Negatives, that means that some employees may be identified as not at risk of leaving when in fact they will leave. But this is still a strong model","metadata":{}},{"cell_type":"markdown","source":"## Feature importance\nwe'll use plot_importance function to inspect the most important features of our final model","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,9))\nplot_importance(xgb_cv.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:44:11.562046Z","iopub.execute_input":"2023-11-09T13:44:11.562532Z","iopub.status.idle":"2023-11-09T13:44:12.205699Z","shell.execute_reply.started":"2023-11-09T13:44:11.562497Z","shell.execute_reply":"2023-11-09T13:44:12.204010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the most importans feature is satisfaction level as expected. But promotion in the last 5 years is not important feature but i thought it has high value on either employee leave or no. XGBoost model couldn't make more use of the department feature.","metadata":{}},{"cell_type":"markdown","source":"## Summary of model results\n","metadata":{}},{"cell_type":"code","source":"print(results)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:45:07.375863Z","iopub.execute_input":"2023-11-09T13:45:07.376335Z","iopub.status.idle":"2023-11-09T13:45:07.386603Z","shell.execute_reply.started":"2023-11-09T13:45:07.376300Z","shell.execute_reply":"2023-11-09T13:45:07.385189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After conducting feature engineering and modeling two different models we have our champion model - XGB\nthat achived precision-score of 95.1%, recall-score of 93.2%, f1-score of 94,2% and accuracy-score of 98%","metadata":{}},{"cell_type":"markdown","source":"## Conclusion, Recommendations\nThe models and the feature importance extracted from the models confirm that employees at the company overwork a lot.\n\nTo retain employees company could change their working politics such as:\n* Cap number of project that employee can work on\n* Reward employees that overwork or create enviroment so that they couldn't overwork.\n* Consider promoting employees who have been working for the company at least four years, or investigate why their satisfaction level is so low.","metadata":{}}]}